% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{hyperref}
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
\usepackage{float}
\usepackage{subcaption}
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%\urlstyle{rm}
%
\usepackage{soul}
\usepackage[dvipsnames]{xcolor}
\usepackage[commandnameprefix=always,todonotes={textsize=footnotesize}]{changes} % Add 'final' for submission
\sethighlightmarkup{\IfIsColored{{\sethlcolor{authorcolor!30}\hl{#1}}}{#1}}
\definechangesauthor[name={Upal Bhattachara}, color=Violet]{UB}

\usepackage[numbers,sort&compress]{natbib}
\bibliographystyle{splncs04nat}

\usepackage{amsmath}
\usepackage{multirow}
\newcommand*{\hcalignbf}[1]{\multicolumn{1}{|c|}{\textbf{#1}}}
\newcommand*{\hlcyan}[1]{{\sethlcolor{Cyan}\hl{#1}}}
\newcommand*{\hlgreen}[1]{{\sethlcolor{YellowGreen}\hl{#1}}}
\newcommand*{\hlorange}[1]{{\sethlcolor{Apricot}\hl{#1}}}

\usepackage{listings} % For better verbatim and code highlighting
\lstset{
  basicstyle=\ttfamily,
  escapeinside=||,
  breaklines=true,
  breakautoindent=false,
  breakindent=0pt,
  numbers=none
}

\begin{document}
%
\title{Ontology Population Using LLMs: Which Factors Matter?}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Upal Bhattacharya\inst{1}\orcidID{0000-0002-0877-7063}\and
        Maaike de Boer\inst{2}\orcidID{0000-0002-2775-8351} \and
        Sergey Sosnovsky\inst{1}\orcidID{0000-0001-8023-1770}}

\authorrunning{U. Bhattacharya et al.}
% % First names are abbreviated in the running head.
% % If there are more than two authors, 'et al.' is used.
% %
\institute{
    Department of Information and Computing Sciences, Utrecht Univerisity, Princetonplein 5, 3584 CC, Utrecht, The Netherlands \\
    \email{u.bhattacharya@uu.nl, s.a.sosnovsky@uu.nl} \and
    Department Data Science, TNO, Anna van Buerenplein 1, 2595 DA, Den Haag, The Netherlands \\ 
    \email{maaike.deboer@tno.nl}
    }
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
The abstract should briefly summarize the contents of the paper in
150--250 words.

\keywords{First keyword  \and Second keyword \and Another keyword.}
\end{abstract}
%
\section{Introduction}
\label{sec:introduction}
Manual population of ontologies is a time and effort-intensive process
requiring significant expertise in a target domain and knowledge engineering.
Automated ontology learning aims to expedite this process by extracting
knowledge components (terms, concepts, relations between them) from
unstructured and semi-structured data.  The wealth of topical information that
Large Language Models (LLMs) can produce on demand has led to the growing
interest in the Semantic Web Community to adopt LLMs for various ontology
enhancement
\cite{dong2024LanguageModelBased,mateiu2023OntologyEngineeringWith,
  he2023ExploringLargeLanguage,liu2020ConceptPlacementUsing,
  funk2023TowardsOntologyConstruction} and ontology learning
\cite{he2022BertmapBertBased,chen2023ContextualSemanticEmbeddings,
  he2023LanguageModelAnalysis,babaei2025Llms4omMatchingOntologies} tasks.
However, recent studies highlight that such affordance of information does not
automatically translate into strong performance of LLMs on all ontology
learning tasks \cite{bombieri2024DoLlmsDream,wang2024CanLargeLanguage}.

Understanding the specific conditions under which an LLM performs well on an
ontology learning task is vital to obtain the most reliable performance from
these models. Ontology population is one such task that requires LLMs to
exhibit their understanding of individual-to-concept assertions and concept
hierarchies. To provide a better understanding of the underlying factors, we
investigate the effect of several ontology and LLM factors on ontology
population: the task of adding new individuals to an ontology through concept
assertions.  We investigate the effect of the ontology factors: structure,
taxonomy and entity labels and the LLM factors: LLM choice, prompting
approach, domain contextualization and response variation over 3 ontologies
using 4 state-of-the-art LLMs.

The rest of the paper is structured as follows. In the next section, we
highlight related work on ontology population and ontology evaluation.  In
Section \ref{sec:factors} we provide an overview of the different factors that
influence ontology population. Section \ref{sec:experimentation} outlines the
factors we investigate in the present work and our experimental setup. Section
\ref{sec:results} discusses the observed results and Section
\ref{sec:conclusion} concludes the paper.  We provide all the necessary
materials required to reproduce our work in an
\href{https://anonymous.4open.science/r/llm_ontology_awareness-91C4/}{Anonymous
  GitHub Repo}.

\section{Related Work}
\label{sec:related-work}

The field of ontology learning has primarily focused on (semi-)automatically
creating (part of) ontologies.  Ontology enrichment and ontology population
can also be viewed as part of the ontology learning process
\cite{petasis2011OntologyPopulationEnrichment} with many overview and survey
papers being published
\cite{buitelaar2005OntologyLearningText,zhou2007OntologyLearningState,
  wong2012OntologyLearningText,asim2018SurveyOntologyLearning,
  khadir2021OntologyLearningGrand,du2024ShortReviewOntology} on the topic.
Whereas earlier approaches combine traditional linguistic and statistical
approaches, LLMs are often used today \cite{du2024ShortReviewOntology}. One of
the latest developments is the LLMs4OL paradigm
\cite{giglou2024Llms4ol2024Overview} and the accompanying challenge launched
in 2023.  In works related to the challenge, different LLM model families are
tested on several ontology learning tasks such as term typing, taxonomy
discovery, and extraction of non-taxonomic relation
\cite{giglou2023Llms4olLargeLanguage}.

\subsection{Ontology Population}

Ontology population is the task of adding new instances to an ontology
\cite{petasis2011OntologyPopulationEnrichment}.  State-of-the-art approaches
often involve applying natural language processing algorithms like name entity
recognition to extract concepts and related instances from a provided set of
documents and an ontology
\cite{petasis2011OntologyPopulationEnrichment,lubani2019OntologyPopulationApproaches}.
Redundancy and entity disambiguation pose several challenges for ontology
population.  LLM-based approaches include SPIRES
\cite{caufield2024StructuredPromptInterrogation} where a zero-shot method is
used for knowledge base population, and a preliminary work using a combination
of text summarization, retrieval-augmented-generation and prompt engineering
\cite{norouzi2024OntologyPopulationUsing}.

\citet{sahbi2025SemanticVs} compare a traditionally semantic approach and an
LLM-based approach for ontology population in French. The results illustrate
that the semantic approach is consistent and logically coherent, but creates
redundancy, while the LLM creates no redundancies but is not always
consistent.

\subsection{Ontology Evaluation}

Ontologies themselves can be evaluated in many different ways and on many
different levels \cite{ wilson2022OntologyQualityEvaluation}.  As described in
\citet{brank2005SurveyOntologyEvaluation}, evaluation of ontology learning,
enrichment and population can be categorized into four main types:
data-driven, human-based, golden standard-based and, application-based
(sometimes described as a `black box' approach
\cite{mcdaniel2019EvaluatingDomainOntologies,hlomani2014ApproachesMethodsMetrics}).
Metrics include the calculation of the 1) syntactic quality on the formal
properties of the knowledge graph such as the inheritance relationships, rules
and richness of the ontology, 2) semantic quality on the information about the
domain such as the interpretability, completeness and consistency, 3)
pragmatic quality about the usage of the ontology such as the
comprehensiveness, ease of use and adaptability and 4) social quality in terms
of authority, history and recognition
\cite{mcdaniel2019EvaluatingDomainOntologies}.  An extension / modification of
these metrics can be found in \citet{wilson2023ConceptualModelOntology} and
\citet{bakker2024DynamicKnowledgeGraph}.

% Ontology Evaluation
The variability introduced by LLM-based approaches requires exploring
additional evaluation metrics.  There has been considerable interest in
developing reliable and robust evaluation setups to evaluate language models
for ontology-based tasks
\cite{bombieri2024DoLlmsDream,he2023ExploringLargeLanguage}.For example,
\citet{babaei2023Llms4olLargeLanguage} evaluate LLM performance on term
typing, taxonomy discovery and relation extraction in a zero-shot
setting. Testing on nine different datasets, it provides a wide gamut of
information about the effects of size and complexity of ontologies on such
tasks. In their work on gauging how well LLMs actually learn reasoning through
concept relations, \citet{mai2024DoLlmsReally} note inconsistencies suggesting
that LLMs tend to fall back to their pre-learnt lexical senses as opposed to
using the provided semantic meanings of concepts in ontologies.

% End
Despite growing interest in such methods of evaluation, insufficient attention
is paid to the underlying LLM-specific and ontology-specific variables. The
interplay of these variables is significant as highlighted by variation in
reported performance across the domain.  In the next section, we outline the
relevant LLM-based and ontology-based variables that may contribute to
performance variation on ontology learning and enrichment tasks. The influence
of these underlying factors can provide strong evidence about the competency
of LLMs on various ontology learning and enrichment tasks.

\section{Factors}
\label{sec:factors}

The factors governing LLM-supported ontology population can be categorized
into two groups: Ontology factors and LLM factors. We provide an overview of
the relevant factors for each group, highlighting their relevance in
LLM-supported ontology population. Figure \ref{fig:variation-overview}
provides an overview of the factors of each group.

\begin{figure}[H]
  \includegraphics[width=\textwidth]{assets/factor-overview.drawio.pdf}
  \caption{Overview of Ontology and LLM factors influencing LLM-supported
    Ontology Population. Dashed boxes are not investigated in the present
    study} \label{fig:variation-overview}
\end{figure}

\subsection{Ontology Factors}
\label{subsec:ontology-factors}

Ontologies vary greatly depending on the complexities of the modeled domain
and the design choices taken by ontology engineers and domain experts
\citep{noy2001OntologyDevelopment101}. Our categorization of ontology factors
draws inspiration from the Ontology Learning Layer Cake
\citep{gangemi2005OntologyEvaluationValidation} with additional included
factors that are not captured by it.

\subsubsection{Scope:}
\label{subsubsec:ont-factor-scope}

The scope of an ontology defines the level of specificity and abstraction of
its entities and its structural design. Ontologies can be \textbf{upper-level}
ontologies that define abstractions enabling integration of heterogeneous
knowledge across different domains
\citep{mascardi2007ComparisonUpperOntologies} or of a particular
\textbf{domain} itself e.g. the Gene Ontology (GO)
\cite{ashburner2000GeneOntologyTool} designed using the principles provided by
an upper ontology. The scope of ontologies test the semantic abilities of
LLMs. Upper ontologies challenge LLMs to understand abstractions effectively
requiring them to extract the ontological semantics of entities whereas domain
ontologies require LLMs to more directly apply their 'understanding' of
entities. The former requires LLMs to `take a step back' from the actual
content and recognize general patterns while the latter requires understanding
patterns based on the content itself.

\subsubsection{Metrics:}
\label{subsubsec:ont-factor-metrics}

% Overall

\citet{hlomani2014ApproachesMethodsMetrics} highlight several
\textbf{structural} and \textbf{functional} metrics for evaluating ontologies.
\begin{itemize}
\item Structural metrics evaluate the structural complexity of an ontology in
  terms of its size, breadth, depth and dispersion
  \citep{hlomani2014ApproachesMethodsMetrics}. The structure of an ontology
  highlights the semantics of the underlying domain or scope. The breadth and
  size of an ontology in terms of its entity count is an indicator of the
  vastness of its scope. Ontologies with large depth reflect domains with
  knowledge of high granularity while those with greater dispersion represent
  domains with several closely-related concepts (siblings). Structural nuances
  test the ability of LLMs to identify and populate domains of varying
  enormity and relational complexity.
\item Functional measures test the intended use of an ontology
  \citep{gangemi2005OntologyEvaluationValidation}. They evaluate the logical
  consistency and comprehensiveness of an ontology as a specification of a
  domain and its conceptualization. Measures likes completeness, conciseness,
  coverage and clarity \citep{hlomani2014ApproachesMethodsMetrics} indicate
  functional competency of an ontology in modelling its domain. While these
  measures are approximations of the effectiveness of the modelling of the
  domain, they can help highlight whether functional adequacy of an ontology
  influences the ability of an LLM to identify its assertions.
\end{itemize}

\subsubsection{Entity Labels:}
\label{subsubsec:ont-factor-entity-labels}

The naming of entites in an ontology is largely governed by conventions
followed in the domain of interest and by any upper ontologies used as
reference. Through their pre-training, LLMs exhibit impressive natural
language understanding and generation capabilities. The lexical semantics
learnt by LLMs are known to exhibit impressive performance on tasks of sense
distinction and semantic equivalence
\citep{hayashi2025EvaluatingLlmsCapability,petersen2023LexicalSemanticsWith}.
However, the ``understanding'' LLMs develop of words is dependent on the
amount of their pre-training data that relates to it
\cite{kandpal2023LargeLanguageModels} (i.e. the popularity of the words and
topics in the pre-training data). Variation in entity label naming highlights
the parity between the ontological semantics of the entities and the learnt
lexical semantics of LLMs for their labels. This parity is crucial for
effective ontology learning. As most LLMs employ subword tokenization like
Byte Pair Encoding (BPE) \citep{sennrich2016NeuralMachineTranslation} for
understanding prompts and generating texts, evaluation over entity labels
highlights whether LLMs are able to effectively utilize lexical semantics or
resort to more conventional string edit-distance-based senses for performing
ontology learning.

\subsubsection{Taxonomy:}
\label{subsubsec:ont-factor-taxonomy}

The taxonomy of an ontology is the backbone structure that defines the
hierarchy of concepts in a domain. While ontology population is concerned
primarily with identifying instances for concepts, it requires understanding
the granularity of the taxonomy to identify the correct concept for a new
individual assertion. Through LLM-supported ontology population, we can
investigate whether LLMs are capable of understanding the taxonomy of an
ontology and how new individuals are to be placed within that taxonomy.

\subsubsection{Non-hierarchical Relations:}
\label{subsubsec:ont-factor-other-relations}

Apart from hierarchical relations, ontologies possess a wide variety of other
relations that describe other phenomena of a domain. Parthood, associative
relations and attribute relation properties comprise a large portion of
ontological relations. Such relations add complexity to the task of ontology
population, requiring care to avoid asserting new individuals with closely
related concepts e.g. asserting `Red' as an instance of the concept `Wine' in
the Wine Ontology \citep{noy2001OntologyDevelopment101} instead of an
attribute value of `WineColor' that is associated with different classes of
wine with the relation `hasColor'. Analysing whether LLMs are susceptible to
such pitfalls when populating ontologies can provide insight into their
ability to accurately apply a sense distinction in the context of ontologies.

\subsubsection{Attributes:}
\label{subsubsec:ont-factor-attributes}

Closely related to non-hierarchical relations, attributes and properties pose
challenges of ontology sense distinction for LLMs enabling investigation of
their ability to distinguish attributes and data classes from core concepts of
the domain.

\subsubsection{Axioms:}
\label{subsubsec:ont-factor-axioms}

Axioms are the fundamental underlying principles that define the theory and
capabilities of an ontology. Axioms of logic, classes, individuals, properties
and classes define what is true in an ontology. They are logical abstractions
of the behaviour exhibited by the entities of an ontology in the other layers
of the Ontology Learning Layer Cake. While ontology population itself does not
strongly concern itself with the axioms of an ontology, behaviour observed
over the other factors observed for LLM-supported ontology population provide
indicators of the compliance of LLM-based reasoning for ontology population
with ontology axioms.

\subsection{LLM Factors}
\label{subsec:llm-factors}

The capabilities of LLMs in natural language generation and understanding has
led to immense growth in research about their abilities and limitations. As an
ever-growing field, factors regularly change with new research. We categorize
LLM factors based on already well-established factors in LLM research.

\subsubsection{Choice of LLM:}
\label{subsubsec:llm-factor-choice}

The number of available LLMs is growing constantly with HuggingFace
\cite{wolf2019HuggingfacesTransformersState} now reporting over $2.2$ million
models. The choice of an LLM is one of the first factors influencing ontology
population. Choosing an appropriate LLM is governed by several additional
factors:
\begin{itemize}
\item The emergent abilities of LLMs grows with \textbf{model size}
  \cite{chalmers2023CouldLargeLanguage}. Thus, it may be desirable to use
  larger LLMs for ontology learning tasks. However, while LLMs are accurate on
  topics of greater popularity, they are known to struggle and hallucinate
  with long-tail knowledge \cite{kandpal2023LargeLanguageModels}. In such
  situations, choosing a larger model itself is insufficient to alleviate the
  issue. When in-context learning is unable to peform well, fine-tuning is
  known to provide significant performance benefits
  \cite{liu2022FewShotParameter}. Owing to their size, larger LLMs may be
  difficult to fine-tune due to computational resource limitations. As a
  result, when working with ontologies in the long-tail of LLMs, fine-tuning
  smaller LLMs might provide superior performance. Comparison of performance
  of larger LLMs versus their smaller counterparts with fine-tuning highlights
  whether larger and costlier models are essential for LLM-supported ontology
  learning or smaller models fine-tuned for specific tasks are sufficient.
\item The \textbf{availability} of model weights enables task-specific
  fine-tuning and further analysis of performance. However, commercially
  available state-of-the-art LLMs operate as `black-boxes' that prevents
  further analysis of performance. In contrast, open-source and open-weight
  models provide model alternatives that can be fine-tuned for long-tail
  domain-specific ontology population. While commercially-available LLMs are
  known to perform better \cite{bommasani2023HolisticEvaluationLanguage},
  comparing performance of open models against commercial LLMs fosters
  research to bridge the gap in performance across all natural language
  processing (NLP) tasks.
\item LLMs are capable of handling a wide variety of tasks. However, they do
  struggle with \textbf{specialization} in reasoning. Ontology learning with
  LLMs requires a good understanding of ontological entity types, taxonomies
  and relations with several tasks requiring multi-hop reasoning.
  Investigation of performance of general-purpose LLMs versus
  reasoning-specific LLMs highlights the significance of LLM reasoning
  capabilities for ontology learning. In particular, it probes the importance
  of using specific reasoning-based models to perform nuanced ontology
  learning.
\end{itemize}

\subsubsection{Objective Modelling:}
\label{subsubsec:llm-factor-objective-modelling}
LLMs are used for a broad set of tasks that can be modelled as various NLP
objectives. Instruction-tuned LLMs are `prompted' with a task description and
a response format that complies with the modelled objective. As models that
use in-context learning to understand the task from the input prompt, the
formulation of the objective is a factor for any ontology learning
objective. Modelling an ontology learning task as classification, retrieval or
another objective results in varied responses generated by LLMs and, as a
result, varied performance. Varied objective modelling highlights the
sensitivity LLMs exhibit to different objective approaches e.g. if an LLM is
better at performing taxonomy learning as a binary classification objective
versus as a ranked retrieval objective.

\subsubsection{Approach:}
\label{subsubsec:llm-factor-approach}

LLMs can be utilized as instruction-following agents, fine-tuned for a
specific objective or coupled with a data store to facilitate
data-substantiated text generation. Comparison of performance across different
approaches highlights the relevance of pre-learnt information, task-specific
augmentation and data availability for ontology learning.

\begin{itemize}
% Prompting
\item A significant number of LLM approaches focus on \textbf{prompting}
  strategies. Prompt engineering has itself become an area of research
  \cite{liu2023JailbreakingChatgptVia,mosbach2023FewShotFine,
    qiao2023ReasoningWithLanguage,schulhoff2024PromptReportSystematic,
    white2023PromptPatternCatalog} due to its openness and lack of consensus
  apart from general principles\footnote{
    \href{https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api}
    {Best Practices for Prompt Engineering with OpenAI API}} for structuring
  prompts. Taking advantage of the in-context learning capabilities of LLMs,
  prompting methods focus on strategies that require the LLM to either perform
  a task without any example (zero-shot), to utilize illustrative examples
  with the intention of better task alignment (few-shot) or, by sequentially
  breaking down a task into a sequence of steps allowing LLMs to reason better
  (Chain-of-Thought \cite{wei2022ChainThoughtPrompting}). These strategies
  indicate the relevance of the pre-learnt knowledge of LLMs for ontologies
  and their ability to adapt when given additional context.
% Fine-tuning
\item While LLMs are capable of performing several tasks `as-is',
  task-specific \textbf{fine-tuning} provides an effective strategy to improve
  performance when in-context learning is unable to yield improvements
  \cite{liu2022FewShotParameter}.  As mentioned previously, domain and
  task-specific \textbf{fine-tuning} provides performance improvements when
  prompting-based approaches are insufficient. When working with information
  in the long-tail, fine-tuning allows LLMs to adapt better than with
  in-context learning. It facilitates ontology learning over ontologies that
  are in the long-tail of an LLM's pre-training data. 
% RAG
\item \textbf{Retrieval-augmented Generation (RAG)}
  \cite{lewis2020RetrievalAugmentedGeneration} provides LLMs with an external
  knowledge base to utilize as factual grounding for generating responses. It
  aims to counter hallucinations \cite{huang2025SurveyHallucinationIn} and
  provide greater domain specificity through the knowledge source context. RAG
  for ontology learning highlights how LLM performance improves when providing
  a specific domain knowledge source. This performance is of particular
  interest when working with ontologies in the long-tail. RAG models provide
  a cost-effective alternative to fine-tuning while still operating on the
  same objective of performance improvement and response factuality.
\end{itemize}

\subsubsection{Domain Context:}
\label{subsubsec:llm-factor-domain-context}
LLMs can be prompted to assume certain roles/personas e.g. `helpful assistant'
while performing a certain task. These roles define a domain space for the LLM
and is a powerful tool that contextualizes the task and user input data for
the LLM. For ontology learning, the specificity of this domain context can be
varied to emphasize the ontological nature of the data, the topic of the
ontology or generalized to the modelling objective. As a factor, domain
context analyzes the amount of data domain context necessary for an LLM to
perform an ontology learning task.

\subsubsection{Response Variation:}
\label{subsubsec:llm-factor-response-variation}

LLMs are non-deterministic based on their probabilistic generation
strategy. Consistency and reliability of responses is paramount in ontology
learning. As a consequence, assessing the variation in responses generated by
LLMs is crucial to understanding their suitability for ontology learning.
\begin{itemize}
\item The determinism, length and repetitiveness of LLM responses can be
  controlled using a wide variety of hyperparameters. \verb|top_k| and
  \verb|top_p| influence the choice of tokens selected by confining the
  sampling space to a fixed number of tokens based or based on a cumulative
  frequency. \verb|frequency penalty| to penalize repetition and reward
  response variation. \verb|temperature| performs simulated annealing of the
  token probabilities and controls randomness of responses. Lower values
  emphasize deterministic behaviour while higher values encourage
  creativity. The various hyperparameters highlight the importance of variety
  and token-constrained response generation for LLMs to perform ontology
  learning.
\item In conjunction with the various hyperparameters, statistical consistency
  of LLM responses across repetitions is necessary for their ability to
  perform ontology learning. Statistical consistency displays whether LLMs are
  a good fit for development of logical and factual knowledge systems like
  ontologies.
\end{itemize}

\subsection{Interplay of Factors}
\label{subsec:factor-interplay}

Ontology and LLM factors influencing LLM-supported ontology learning are not
independant of each other. These factors influence each other in complex ways
thereby introducing additional complexity in their effect on an ontology
learning task. Ontology factors like structure are intrinsically
linked to entities in an ontology and therefore to the OL Layer Cake of
ontology factors. Functional measures of an ontology are driven by the
taxonomical and non-hierarchical modeling of the underlying domain. At the
base level, the scope of an ontology is a guiding factor determining the
existence of certain entity types and the nature of permissible
attributes. In terms of LLM factors, the choice of approaches may be confined
to certain LLMs depending on their size or the existence of fine-tuning APIs
for commerical models. Different domain contexts may influence the
hyperparameter settings that enable accurate ontology learning. Reasoning
specialization models may not be suitable for certain approaches while certain
objective modelling approaches may benefit from different domain contexts.

The complex inter-factor influence affecting LLM-supported ontology learning
adds several levels of complexity to accurately understand how each factor
acts in isolation. Effective marginalization of each factor and comparing
performance by varying inter-related variables provides insight into the exact
manner of influence each factor introduces.

\section{Experimentation}
\label{sec:experimentation}

To illustrate the influence of various factors on an LLM-supported ontology
learning task, we perform experiments for ontology population and investigate
the ontology factors: structure, taxonomy and entity labels; and the LLM
factors: choice of LLM, modelling objective, prompting approach, domain
context and response variation.

\subsection{Ontology Population}
\label{subsec:ontology-population}

Ontology population is the task of mapping new individuals to concepts within
an ontology \cite{petasis2011OntologyPopulationEnrichment}. We select ontology
population as the first learning task to investigate as it is the simplest of
all ontology learning tasks. Despite being simple, ontology population
requires an LLM to ``understand'' concepts and instances in an ontology to
make new assertions. LLM-supported ontology population helps us investigate
the degree of awareness LLMs posses of ontology assertion relations and of the
entities themselves.

% \subsection{Formulation}
% \label{subsec:formulation}

{\parindent0pt % disable indentation for formulation
  Following \citet{hlomani2014ApproachesMethodsMetrics}, we define an ontology
  $O$ as a 4-tuple:
  \begin{align}
    O = \langle C, H, R, A \rangle
  \end{align}

  where, \newline $C$ is the set of Concepts $\{c\}$; \newline $H$ is the set
  of taxonomical/hierarchical (`is a') relations over $C$; \newline $R$ is the
  set of non-taxonomical relations over $C$ and; \newline $A$ is a set of
  axioms \newline

  Let $t$ be a term to be mapped as an individual to a concept in $C$ \newline
  Then the task of ontology population is:
  \begin{equation}
    f(t;O)^{Ontology}_{Population} =\ \{c_i \mid c_i \in C\ ;\ c_i \subseteq
    c_{i+1}\ ;\ t \in \Sigma_{c_1}\ ;\ 1 \leq i \leq D - d + 1\}
  \end{equation}
  where, \newline $D$ is the depth of the ontology $O$ \newline $d$ is the
  depth of the concept $c_1$ in $O$ \newline
  $\subseteq$ denotes the `subclass of' or `is a' relation (taxonomical
  relations of $O$ i.e. $H$); \newline $c_1$ is the directly asserted concept
  of the term $t$ and,\newline $\Sigma_{c_1}$ is the extension
  \cite{hlomani2014ApproachesMethodsMetrics} of $c_1$ (i.e. set of asserted
  individuals) }
\subsection{Ontology Factors}
\label{subsec:experiment-ontology-factors}

We select three domain-specific ontologies characterized by varying structural
complexity and, (by virtue of their differing domains) distinct entity labels.
% Dataset Choice
\begin{itemize}
\item The Wines Ontology \cite{noy2001OntologyDevelopment101} is a
  structurally simple and well-known ontology regarding wines. It represents
  information about the colour, flavour and origin of various wines. The Wines
  Ontology provides a simple ontology learning use-case that we expect most
  LLMs to have good knowledge of either directly or through the topic of
  wines. 
\item The CASE Ontology \cite{casey2017AdvancingCoordinatedCyber} is a larger,
  more complex and newer ontology focused on accurately capturing the
  life-cycle of digital evidence. We incorporate individuals from the Owl
  Trafficking example available on the CASE
  website\footnote{\url{https://caseontology.org/examples/}} to investigate
  ontology population with the ontology. As the CASE Ontology is built on top
  of the UCO Ontology \cite{casey2018EvolutionExpressingExchanging}, we
  include concepts from it in our experimentation. Hereafter, we refer to this
  composite constructed ontology as the CASE Ontology. The CASE ontology poses
  a complex and lesser known scenario that LLMs may struggle with.
\item The Astronomy Ontology \cite{shaya2012AstronomyOntology} is the largest
  ontology of the three ontologies concerning astronomical phenomena including
  planets, stars and the relevant laws of physics. Despite its large size, the
  Astronomy Ontology concerns a well-known topic that one might expect LLMs to
  have `seen' abundantly in its pre-training corpus.
\end{itemize}
The chosen ontologies do not include relational loops and are of single
parentage.
% We wish to expand the considered set of ontologies to include larger
% ontologies like DBPedia \cite{auer2007DbpediaNucleusWeb} with more complicated
% relationships and hierarchies in future works.  Table
% \ref{tab:ontology-metrics} outlines relevant structural metrics of the
% ontologies to highlight their differences.

\subsubsection{Structure:}
Table \ref{tab:ontology-structure-metrics} provides an overview of the
structural metrics of the three ontologies.  In terms of size, the Astronomy
Ontology has a significantly larger number of concepts. In contrast, the
smallest ontology: Wines, has the largest number of individuals.
% The Astronomy Ontology has the largest breadth
% \cite{gangemi2005OntologyEvaluationValidation} which is an indicator of the
% diverseness of its domain.
The CASE Ontology is characterized by larger maximum dispersion
\cite{gangemi2005OntologyEvaluationValidation}, highlighting the existence of
a large variety of sibling concepts of similar granularity of the domain. The
structural differences between the three ontologies can help identify the
macroscopic structural design implications on ontology population. Greater
depth signifies higher information granularity that can pose challenges of
semantic separation between deeper entities and their parents.
% Broader ontologies exhibit greater information diversity that can confuse
% LLMs about the relevant domain of entities.
Dispersion evaluates semantic separability for closely related sibling
entities. We investigate correlations between LLM-supported ontology
population performance and dispersion and depth of concepts in all three
ontologies. In particular, we investigate if concepts of greater depth or with
a larger number of child concepts are more difficult for LLMs to make accurate
predictions for.
% The three ontologies represent a very small, a medium and a relatively large
% ontology to test for ontology population. However, in compared to larger
% ontologies like DBPedia \cite{auer2007DbpediaNucleusWeb}, all three are
% relatively small and structurally simple.

\begin{table}[H]
  \centering
  \caption{Ontology Structure Metrics for Wines, CASE and Astronomy
    ontologies}
  \label{tab:ontology-structure-metrics}
  \begin{tabular}{|l|r|r|r|}
    \hline
    \textbf{Metric}                                                                & \textbf{Wines} & \textbf{CASE} & \textbf{Astronomy} \\
    \hline
    \textbf{Classes (no.)}                                                    & 76                      & 434                    & 1663                        \\
    \textbf{Individuals (no.)}                                                & 161                     & 131                    & 68                          \\
    \textbf{Depth} \cite{gangemi2005OntologyEvaluationValidation}             & 4                       & 8                      & 10                          \\
    \textbf{Breadth} \cite{gangemi2005OntologyEvaluationValidation}           & 62                      & 228                    & 989                         \\
    \textbf{Dispersion (max.)} \cite{gangemi2005OntologyEvaluationValidation} & 3                       & 118                    & 44                          \\
    \hline
  \end{tabular}
\end{table}

\subsubsection{Taxonomy:} An understanding of the taxonomy of an ontology
indicates a strong level of ontological awareness. While ontology population
itself does not directly investigate ontology taxonomy, incorporating
taxonomical investigation in the ontology population task provides insight if
LLMs perform ontology population with an entity relation approach that matches
ontology hierarchies. An ability to identify directly-asserted concepts for
individuals but a failure to identify taxonomies highlights poor understanding
of ontology taxonomies and therefore, poor ontological `awareness'. To
investigate this, our designed modelling objective for ontology population
(Section \ref{subsec:experiment-llm-factors}) requires LLMs to identify not
just directly-asserted concepts but hierarchies as well for more robust
investigation.

\subsubsection{Entity Labels:}
% Embedding Similarity
The domain differences between the three ontologies introduce entity labelling
differences. We investigate the parity between ontological label semantics and
LLM lexical understanding using similarity measurement between LLM embeddings
of individuals and concepts. As instance realizations of concepts, individuals
should be semantically closely related with their asserted concepts.

We generate 3072-dimensional embeddings of each individual and concept label
for all three ontologies using OpenAI's \verb|text-embedding-3-large|
\cite{openai2024EmbeddingModels} model. Despite the embeddings being primarily
representative of the OpenAI models, the observed performance trends are
similar across all tested models (Section \ref{sec:results}), which lead us to
believe that the results can be generalized beyond the GPT family. Through
cluster separation analysis of the embeddings, we analyze if the LLM
representations exhibit higher separation between entity and concept
labels. In particular, we investigate if LLM representations have high
separation between concept and individual labels which can result in poor
ontology population performance.

We supplement the analysis of entity labels by investigating the lexical
matching between concepts and individuals. This supplementary analysis aims to
assess whether LLMs can resort to simpler string similarity-based ontology
population. The naming conventions of entities in ontologies can lead to
similar but distinct concepts possessing lexically similar labels. This can
cause confusion for LLMs if they resort to string matching for ontology
population. We evaluate the string edit-distance between individual labels and
two groups: 1) the directly-asserted concept of the individual and, 2) all
other ancestor concepts of the individual (averaged). The combination of the
two highlights the ability of an LLM to identify a concept assertion from the
directly-asserted concept and individual labels versus its ability to
additionally predict other ancestors using lexical matching. Based on our
formulation of the modelling objective (Section
\ref{subsec:experiment-llm-factors}), the second distance provides an
indicator of the semantics-based ontology population that LLMs are required to
employ in order to be robust.

\subsection{LLM Factors}
\label{subsec:experiment-llm-factors}

Our experimentation involves several LLMs, a modelling
objective that expounds an LLM's ontological taxonomical understanding,
multiple prompting approaches, domain contexts and an investigation of
response consistency over temperature variation.

\subsubsection{Modelling Objective:}

Following a similar investigation of term typing in
\citet{giglou2024Llms4ol2024Overview}, we model the task of ontology
population as a retrieval problem requiring LLMs to generate a ranked list of
concepts of length up to the depth
\cite{gangemi2005OntologyEvaluationValidation} of the ontology. We provide
minimal context about the ontology from which individuals and concepts are
taken to force the LLMs to utilize their own knowledge. This choice highlights
the relevance and utility of an LLM's "world knowledge" for a basic ontology
learning task.  Requiring an LLM to retrieve concepts up to the depth of the
ontology provides insight into their ability to infer concept hierarchies when
only provided with concept labels. It illustrates better ontological
understanding.

% \subsection{Evaluation Metrics}
% \label{subsec:evaluation-metrics}
We evaluate using the standard information retrieval metric Mean Average
Precision (mAP) \cite{baezayates1999ModernInformationRetrieval} with mAP
computed at 1 ($mAP@1$) highlighting the ability of LLMs to identify the
directly-asserted concept for an individual and at the depth $D$ ($mAP@D$) of
each ontology to better understand an LLM's ability to infer the correct
concept hierarchies.

For each individual, the ground truth is defined as the sequence of concepts,
starting at the individual's directly asserted concept, along the path to the
top concept of that ontology.  We provide a mathematical formulation of
$\text{mAP@K}$ with $K=1,D$ being applied to obtain $\text{mAP@1}$ and
$\text{mAP@D}$ respectively.
\newline \\
% {\parindent 0pt
Let the ground truth hierarchy for an individual $t$ be given by:
\begin{equation}
  \label{eq:ground-truth}
  \{c_i \mid c_i \in C\ ;\ c_i \subseteq c_{i+1}\ ;\ t \in \Sigma_{c_1}\ ;\ 1
  \leq i \leq D - d + 1\}
\end{equation}
where $d$ is the depth of the directly-asserted concept $c_1$ and $d \leq D$ \\
Let $\{y_i\}_{i=1}^{D-d+1}$ be the predicted hierarchy for $t$ \\
The mAP at a sequence length of $K (\leq D-d+1)$ is given by:
\begin{equation}
  \label{eq:map}
  \text{mAP@K} = \frac{1}{\big|T\big|} \sum_{t \in T} \text{AP@K}_t
\end{equation}
where, \\
$T$ is the set of individuals; \\
$| \cdot |$ is set cardinality and; \\
$\text{AP@K}_t$ is the average precision for $t$ at a sequence length $K$,
given by:
\begin{equation}
  \label{eq:ap}
  \text{AP@K}_t = \frac{1}{K} \sum_{x=1}^{K} \frac{\Big|\{y_i\}_{i=1}^{x} \cap \{c_j\}_{j=1}^{x}\Big|}{\Big|\{y_k\}_{k=1}^{x}\Big|} 
\end{equation}
The numerator in Equation \ref{eq:ap} is the cardinality of the set of
\textit{true positives} and the denominator is the cardinality of the set of
\textit{predictions} for a single indvidual $t$.

\subsubsection{Choice of LLM:}
% Choice of LLM
We conduct experiments using four instruction-tuned LLMs: OpenAI's GPT-4o
\cite{openai2024HelloGpt4o}, OpenAI's o1-preview
\cite{openai2024IntroducingOpenaiO1}, Meta's Llama3-8B
\cite{dubey2024Llama3Herd} and DeepSeek's R1-Distil-Llama-8B
\cite{deepseekai2025DeepseekR1Incentivizing}.
% Reasoning vs. Conventional LLMs Closed-source vs. Open-source
Our selection involves two larger commerical models: GPT-4o and o1-preview and
two smaller open-source models: Llama3-8B and R1-Distil-Llama-8B. GPT-4o and
Llama3-8B are general LLMs while o1-preview and R1-Distil-Llama-8B are
reasoning-specific LLMs.

% Prompting strategy
\subsubsection{Prompting approach:}
To investigate the effect of different prompting approaches, we perform
experiments on all ontologies with zero-shot prompts and few-shot prompts. For
few-shot prompts, we experiment with providing different numbers of examples
ranging from 1 to 10. The provided examples are selected randomly from the
concepts with the highest number of individuals, ensuring each example is
taken from a different concept.

\subsubsection{Domain Context:}
We experiment with four types of domain contextualization:
\begin{itemize}
\item Generic: The LLM is only asked to perform a generic task e.g. ranked
  retrieval.
\item Ontology: The LLM is defined as an expert in ontologies.
\item Topic of an ontology: The LLM is defined as an expert in the topic of an
  ontology e.g. \verb|`You are a wine expert'| for the Wines Ontology
  \cite{noy2001OntologyDevelopment101}.
\item Ontology and Topic: The LLM is defined as an expert in ontologies and an
  expert in the topic.
\end{itemize}
Examples of the templates for different prompting approaches and domain
contextualization variations can be found at our
\href{https://anonymous.4open.science/r/llm_ontology_awareness-91C4/}{repository}.

% Temperature variability and response consistency

\subsubsection{Response Variation:}

We evaluate response variation in terms of statistical consistency at
different temperature settings under specific values of different factors.
Our approach involves statistical consistency evaluation of the LLM responses
in terms of mAP at several temperature values: 0.0, 0.2, 0.4, 0.6, 0.8,
1.0. Statistical consistency is measured across 10 repetitions at each
temperature value. Lower temperature values (< 0.5) highlight performance
variation at low response variability while higher values ($\geq$ 0.5)
illustrate performance variation when response variation is encouraged.

% Due to resource restrictions, we were unable to perform similar experiments
% with o1-preview and DeepSeek R1-Distil-Llama-8B and leave it for future
% work. Our choice of the two ontologies is based on the entity label semantics
% of ontologies that we discuss in Section
% \ref{subsec:method-ontology-variability}. The choice of 3-shot prompting is
% based on performance stabilization that we observe from our experiments
% further discussed in Section \ref{sec:results}. We choose to explore
% consistency at the default temperature of each LLM, at a high value and at a
% low value.  The default value for both LLMs is approximately midway in their
% respective ranges. The high and low values are selected by scaling both LLMs'
% ranges to $[0, 1]$ and choosing the equivalent values of $0.2$ (low) and $0.8$
% (high).

% The temperature scales for both the LLMs are different but the default
% values are approximately midway in their respective ranges. For the high and
% low values, we scale both temperature ranges to $[0, 1]$ and choose the
% equivalent values of $0.2$ (low) and $0.8$ (high).


\section{Results and Discussion}
\label{sec:results}
This section presents the results and observations across the 288 conducted
experiments (zero-shot: 4 LLMs $\times$ 3 ontologies $\times$ 4 domain
contextualizations $= 48$; few-shot: 2 LLMs $\times$ 3 ontologies $\times$ 4
domain contextualizations $\times$ 10 few-shot variations $= 240$). Due to
space limitations, we are unable to report all the computed scores here. A
complete table with all the results and figures can be found
\href{https://anonymous.4open.science/r/llm_ontology_awareness-91C4/}{online}
\footnote{\href{https://anonymous.4open.science/r/llm_ontology_awareness-91C4/}{https://anonymous.4open.science/r/llm\_ontology\_awareness-91C4/}}.
Table \ref{tab:results} reports a selection of scores representing the results
for all three ontologies, all four models, zero-shot, 1-shot, 3-shot and
10-shot prompting strategies and the ontology-focused domain
contextualization. The choice to report scores for the ontology domain
contextualization is based on it providing the best results for a larger
subset of our experiments (more detail in Section
\ref{subsec:results-llm-variability}). Despite Table \ref{tab:results}
representing a small portion of the total experiments performed, it still
highlights the nature of the observed trends that we observe across all
variables.
% and the observed trends across all other variables also being evident.
However, when analyzing any factor, we draw our conclusions from all
experiments performed. When analyzing the effects of different factors, we
draw our conclusions with an emphasis on $\text{mAP@D}$ as it better
highlights ontological awareness with the inclusion of taxonomies.

% When analyzing the effect of different variables, we draw our conclusions with
% an emphasis on $\text{mAP@D}$ rather than $\text{mAP@1}$. $\text{mAP@1}$
% focuses only on the ability of an LLM in identifying the directly-asserted
% concept of an individual. However, as discussed in Section
% \ref{subsec:method-ontology-variability}, a large portion of the individuals
% use the label of their directly-asserted concept for their respective
% labels. Such lexical similarity would inflate $\text{mAP@1}$ scores and
% provide an inaccurate representation of the understanding LLMs possess of
% ontologies. $\text{mAP@D}$, in contrast, reduces the emphasis on the
% directly-asserted concept and, by requiring LLMs to generate hierarchies,
% provides greater insight into their understanding of concept hierarchies.

% Macro performance
A macroscopic overview of performance shows that LLMs are quite capable of
performing ontology population when provided with simple labels. LLMs struggle
with zero-shot hierarchy retrieval-based ontology population ($\text{mAP@D}$)
for the CASE and Astronomy ontologies but few-shot context examples largely
help alleviate the issue. The performance differences between the three
ontologies highlight the importance of the domain of the ontologies even on a
simple ontology learning task such as ontology population. In the subsequent
sections, we explore the effects of the outlined Ontology and LLM Factors in
our experimentation.

\begin{table}
  \centering
  \caption{Ontology domain contextualization prediction results at zero-shot,
    1-shot, 3-shot and 10-shot for all ontologies and LLMs
    (DeepSeek-R1-Distil-Llama-8B abbreviated to DeepSeek-R1* to save
    space)}\label{tab:results}
  \begin{tabular}{|c|c|c|r|r|}
    \hline
    \hcalignbf{N-Shot} & \hcalignbf{Ontology} & \hcalignbf{LLM} & \hcalignbf{mAP@1}  & \hcalignbf{mAP@D} \\
    \hline
    \multirow{12}{*}{0}
                       & \multirow{4}{*}{Wines}
                                              & o1-preview & 0.826 & 0.871 \\
    \cline{3-5}
                       & & GPT-4o & 0.733 & 0.785 \\
    \cline{3-5}
                       & & Llama3-8B & 0.484 & 0.571 \\
    \cline{3-5}
                       & & DeepSeek-R1* & 0.472 & 0.542 \\
    \cline{2-5}
                       & \multirow{4}{*}{CASE}
                                              & o1-preview & 0.878 & 0.578 \\
    \cline{3-5}
                       & & GPT-4o & 0.779 & 0.316 \\
    \cline{3-5}
                       & & Llama3-8B & 0.626 & 0.270 \\
    \cline{3-5}
                       & & DeepSeek-R1* & 0.557 & 0.196 \\
    \cline{2-5}
                       & \multirow{4}{*}{Astronomy}
                                              & o1-preview & 0.956 & 0.870 \\
    \cline{3-5}
                       & & GPT-4o & 0.926 & 0.750 \\
    \cline{3-5}
                       & & Llama3-8B & 0.544 & 0.420 \\
    \cline{3-5}
                       & & DeepSeek-R1* & 0.618 & 0.387 \\
    \hline
    \multirow{6}{*}{1}
                       & \multirow{2}{*}{Wines}
                                              & GPT-4o & 0.800 & 0.868 \\
    \cline{3-5}
                       & & Llama3-8B & 0.500 &	0.536 \\
    \cline{2-5}
                       & \multirow{2}{*}{CASE}
                                              & GPT-4o & 0.823 & 0.608 \\
    \cline{3-5}
                       & & Llama3-8B & 0.700 & 0.372 \\
    \cline{2-5}
                       & \multirow{2}{*}{Astronomy}
                                              & GPT-4o & 0.940 & 0.830 \\
    \cline{3-5}
                       & & Llama3-8B & 0.776 & 0.627 \\
    \hline
    \multirow{6}{*}{3}
                       & \multirow{2}{*}{Wines}
                                              & GPT-4o & 0.873 & 0.889 \\
    \cline{3-5}
                       & & Llama3-8B & 0.690 & 0.720 \\
    \cline{2-5}
                       & \multirow{2}{*}{CASE}
                                              & GPT-4o & 0.836 & 0.772 \\
    \cline{3-5}
                       & & Llama3-8B & 0.711 & 0.552 \\
    \cline{2-5}
                       & \multirow{2}{*}{Astronomy}
                                              & GPT-4o & 0.954 & 0.807 \\
    \cline{3-5}
                       & & Llama3-8B & 0.769 & 0.648 \\
    \hline
    \multirow{6}{*}{10}
                       & \multirow{2}{*}{Wines}
                                              & GPT-4o & 0.894 & 0.901 \\
    \cline{3-5}
                       & & Llama3-8B & 0.603 & 0.630 \\
    \cline{2-5}
                       & \multirow{2}{*}{CASE}
                                              & GPT-4o & 0.851 & 0.829 \\
    \cline{3-5}
                       & & Llama3-8B & 0.785 & 0.673 \\
    \cline{2-5}
                       & \multirow{2}{*}{Astronomy}
                                              & GPT-4o & 0.914 & 0.778 \\
    \cline{3-5}
                       & & Llama3-8B & 0.810 & 0.670 \\
    \hline
  \end{tabular}
\end{table}


\subsection{Analysis of Ontology Factors}
\label{subsec:results-ontology-variability}

\subsubsection{Structure:}
To investigate the influence of structure, we conduct a correlation study
between the ability of LLMs to place concepts at the correct position in their
hierarchy during retrieval and ontology population performance.

For any concept $c$ of depth $d$, we compute the Correct Retrieval (CR) as the
percentage of retrievals at the correct hierarchy position across all
individual terms $t \in T$ where the concept was retrieved.
\begin{equation}
  CR(c) = \frac{\big|\{t \in T; c \in \{y_i\}_{t}; y_{D-d+1} = c\}\big|}{\big|\{t \in T; c \in \{y_i\}_t\}\big|}
\end{equation}

CR highlights the ability of LLMs to place concepts at the correct taxonomical
position during ontology population. We compute the Pearson correlation
between CR and two structural metrics: depth and dispersion of concepts. It
highlights whether LLMs struggle with correctly retrieving concepts of greater
depth or dispersion. Using the Pearson correlation values, we compute the
average correlation \cite{corey1998AveragingCorrelationsExpected} across all
96 experiments conducted for each ontology. For all three ontologies, we did
not observe a statistically significant correlation between either depth or
dispersion of a concept with their CR. In terms of size, $\text{mAP@D}$ is the
highest for the smallest Wines Ontology. However LLMs perform better on the
largest Astronomy Ontology compared to the CASE Ontology. The variable
performance based on size and the lack of statistically significant
correlation between correct retrieval and concept structural metrics
highlights that structural metrics do not influence the ability of LLMs to
perform ontology population.

% Strucure
% Depth
% Wines
% t(94) = -0.24, p = 0.01
% Astronomy
% t(94) = 0.03, p = 0.76
% CASE
% t(94) = 0.11 , p = 0.27

% Dispersion
% Wines
% t(94) = -0.30 , p < .05
% Astronomy
% t(94) = -0.17 , p = 0.09
% CASE
% t(94) = -0.13, p = 0.21

% Taxonomy
\subsubsection{Taxonomy:} Comparing $\text{mAP@1}$ and $\text{mAP@D}$ in Table
\ref{tab:results}, we observe that LLMs are adept at inferring direct
assertions during ontology population but can struggle with predicting
hierarchies. This is particularly evident for the zero-shot experiments on the
CASE ontology where $\text{mAP@1}$ is comparable to that of the other two
ontologies but $\text{mAP@D}$ is significantly lower. Addition of examples
through few-shot prompting greatly reduces the divide and inclusion of even a
few examples significantly improves ontology population taxonomy retrieval
(Figure \ref{fig:few-shot}). Providing examples leads to significant
improvements in taxonomy retrieval compared to directly assertion. We observed
that $83$ out of the $161$ individuals ($51.5\%$) for the Wines Ontology, $94$
of the $131$ individuals ($71.7\%$) in the CASE Ontology and, $17$ out of the
$68$ individuals ($25\%$) for the Astronomy Ontology utilize the name of their
directly-asserted concept in their labels. This is unrealistic in a real-world
ontology population scenario. In such circumstances, lexical matching is
sufficient for direct assertion. Thus, $\text{mAP@1}$ provides an inflated
view of the capabilities of LLMs to infer concept assertions whereas
$\text{mAP@D}$ provides a more comprehensive view of their performance
reliability. While ontology population does not concern itself directly with
taxonomies, analysis using $\text{mAP@D}$ provides a more ontologically
grounded evaluation approach. It highlights that LLMs on their own may not
possess good taxonomical sense but in-context learning with few-shot prompting
can largely address this problem.

% Entity Labels
\subsubsection{Entity Labels:} Utilizing the embeddings of concepts and
individual labels, we investigate the influence of entity embedding
separation. We measure entity semantic separation through centroid distance of
embedding cluster of concepts and labels and overall entity semantic
homegeneity using the Davies-Bouldin Index (DBI)
\cite{davies1979ClusterSeparationMeasure} (Table
\ref{tab:entity-label-separation}). We hypothesize that, for LLMs to perform
ontology population effectively, cluster separability between the two types of
entities should be low (i.e a lower centroid distance and a higher DBI is
better). We observe that the normalized centroid distance between concept and
individual clusters for the CASE Ontology to be twice as large as the other
two ontologies (larger values indicate greater inter cluster separation). The
DBI for the CASE Ontology is much lower than the other two ontologies. These
two entity label metrics (Table \ref{tab:entity-label-separation}) together
suggest that the entity labels of the CASE Ontology might make ontology
population difficult for LLMs. The performance in Table \ref{tab:results}
supports our hypothesis with performanve on the the CASE Ontology being
significantly lower than the other two ontologies in the zero-shot scenario
and when using one or two examples for few-shot prompting. Beyond that, the
provision of more examples in few-shot counters the effects of this entity
semantic separation. The semantic separation of the entities of the CASE
Ontology is visually confirmed through a two-dimensional t-SNE
\cite{van2008VisualizingData} projection of the entities of the three
ontologies in Figure \ref{fig:entity-semantic-separation}. The individuals of
the CASE Ontology are well-separated from its concepts in the lower right-hand
corner of the Figure. No such separation is observable for the entities of the
other two ontologies. 

\begin{figure}[H]
  \centering
  \caption{TSNE projection of concepts and individual label embeddings of
    ontologies on 2 components}
  \label{fig:entity-semantic-separation}
  \includegraphics[scale=0.26]{assets/all_entity_semantic_similarity.pdf}
\end{figure}

We measure the lexical similarity between individual and concept labels to
assess whether LLMs can resort to lexical matching to perform ontology
population.  We compute the Levenshtein distance
\cite{levenshtein1965BinaryCodesCapable} between the labels of individuals and
two groups: .  Table
\ref{tab:semantic-lexical-measures} provides the two scores for the three
ontologies. Group 1 disparity is highest for the Wines ontology and lowest for
the CASE ontology. LLMs cannot resort to lexical matching to find the
directly-asserted concept for the Wines ontology but might be able to do so
for the CASE ontology.   Thus, a great deal of lexical matching can be utilized to
identify the directly-asserted concept across the ontologies.  Group 2
disparity clearly highlights that lexical matching is insufficient for LLMs to
infer concept hierarchies.
The combination of the two disparities highlight that LLMs might be able to infer the directly-asserted concepts from lexical matching but cannot infer hierarchies. This plays a crucial role in our decision to formulate the ontology population task as a retrieval task focusing on hierarchies and not just the directly-asserted concept. \\
%
The semantic separation and lexical disparty between the two entity types
indicate that the CASE ontology may pose a harder challenge for LLMs. Semantic
similarity between the entity types for the other two ontologies suggests they
might be easier for LLMs to perform ontology population, even without lexical
similarity.

\begin{table}
  \centering
  \caption{Entity label semantic and lexical metrics for Wines, CASE and
    Astronomy ontologies (Levenshtein shortened to L. for readability)}
  \label{tab:entity-label-separation}
  \begin{tabular}{|l|r|r|r|}
    \hline
    \textbf{Metric}                                                                & \textbf{Wines} & \textbf{CASE} & \textbf{Astronomy} \\
    \hline
    \textbf{DBI} \cite{davies1979ClusterSeparationMeasure}                    & 5.995                   & 2.891                 & 6.362
    \\
    \textbf{Centroid Distance}                                                & 0.259                   & 0.540                  & 0.261
    \\
    \textbf{Direct L. Distance (1)}                                             & 10.285                  & 4.137                  & 5.167
    \\
    \textbf{Ancestor L. Distance (2)}                                   & 12.191                   & 14.092                 & 15.381
    \\
    \hline
  \end{tabular}
\end{table}



%% Observation about ontology population over different ontologies
All experimental variants perform better on the Wines and Astronomy ontologies
compared to the CASE ontology (for mAP@D). This performance disparity is
particularly pronounced in our zero-shot experiments and is significantly
reduced even with the inclusion of a single illustrative example (Table
\ref{tab:results}).

Except DeepSeek's R1-Distil-Llama-8B, all other LLMs report much higher
$mAP@1$ scores than $mAP@D$.  This reinforces the idea that LLMs are unable to
infer the correct concept hierarchies when their semantic understanding of the
entities deviates from that of the ontology. $\text{mAP@1}$ scores for the
CASE ontology are often higher than the other two ontologies. As discussed in
Section \ref{subsec:method-ontology-variability}, a large portion of its
individuals ($71.7\%$) utilize the name of their directly-asserted
concepts. Lower $\text{mAP@D}$ scores for the CASE ontology reinforce the idea
that LLMs resort to lexical matching in identifying directly-asserted concepts
but are unable to correctly infer the concept hierarchies. The semantic
separability of entity types and lack of lexical parity with ancestor concept
labels in conjunction with the poor $\text{mAP@D}$ scores highlight the
difficulty of performing ontology population for the CASE ontology. However,
performance is significantly improved when examples are provided highlighting
the ability of LLMs to quickly grasp the correct taxonomy hierarchy context
required of them.

%%% The other two ontologies
LLMs perform much better on the Wines and Astronomy ontologies.  We observe
higher $map@D$ scores compared to $map@1$ for all experiments on the Wines
ontology.  111 of the 161 individuals in the ontology belong to concepts
without descendants, thus having no hierarchy to predict. Thus, when an LLM is
unable to retrieve the directly asserted concept at the first rank, it is
usually able to retrieve it within the top 4 ranks.  The $map@1$ scores on the
Astronomy ontology are much higher than the rest despite $mAP@D$ being mostly
similar to that of the Wines ontology. As it is a much larger ontology (Table
\ref{tab:ontology-metrics}), we hypothesize that LLMs are able to retrieve the
directly asserted concept but the greater depth of the ontology makes it
slightly difficult to predict its ancestors. However, the $map@D$ scores still
show reasonably accurate results for ontology population and hierarchy
inference. Performance improves further when examples are included. The
similarity in performance between the Wines and Astronomy ontologies despite
their size and structural complexity differences suggests that ontology
structure does not play a major role in ontology learning tasks. However, more
experimentation is required before any definitive conclusion can be
drawn. Agreement between an LLM's semantic interpretation of entity labels and
their ontological nature can be the governing factor. Similar centroid
distance and DBI scores for the Wines and Astronomy ontologies compared to the
CASE ontology support this assumption (Table \ref{tab:ontology-metrics}).

% \subsection{Choice of LLM}
\subsection{Analysis of LLM Factors}
\label{subsec:results-llm-variability}

%% open-source vs. closed-source and large vs. small
We find that larger (closed-source) LLMs outperform their smaller
(open-source) competitors.
% (Tables \ref{tab:zero-shot-results} and \ref{tab:few-shot-results})
Without further experimentation with larger open-source models, it is
difficult to say if other factors beyond the model size are responsible for
this performance gap. We hope to explore this in future work. The performance
gap between GPT-4o and Llama3-8B is very pronounced in the zero-shot setting,
but it strongly reduces in the few-shot experiments. Figures
\ref{fig:gpt-4o-few-shot} and \ref{fig:llama3-few-shot} also highlight that
both models benefit considerably from providing examples to the CASE ontology.
% Despite performance differences between GPT-4o and Llama3-8B, similar trends
% observed across all ontologies in Figure \ref{fig:few-shot} is encouraging.
Although much smaller in size, Llama3-8B manages to reduce the gap in
performance compared to GPT-4o when provided with a handful of examples
(Figure \ref{fig:few-shot}), indicating that even smaller models have the
potential to successfully support ontology population. Similar trends in
performance growth for the CASE ontology further strengthen the assumption of
ontology entity semantics being the principal consideration for LLMs to
perform ontology population.

%% Reasoning vs. Conventional LLMs
o1-preview outperforms all other models in our zero-shot experiments
indicating the potential of reasoning LLMs for ontology-related tasks. Despite
DeepSeek's R1-Distil-Llama-8B being a reasoning LLM , it has the worst
performance among all 4 models. Further prompt engineering could improve
results of the smaller reasoning models, but our experimentation highlights
that reasoning LLMs do not always outperform conventional LLMs. For larger
models, we observe that GPT-4o, when provided with a few examples (Table
\ref{tab:results}), performs as well as o1-preview under zero-shot
conditions. Further experimentation with few-shot prompting reasoning models
would make it clearer if reasoning models are better at supporting ontology
learning; however, recent works
\cite{wang2024Advanced,nori2024Medprompt,guo2025DeepSeek} suggest that
reasoning models do not benefit significantly from few-shot prompting and may
even lead to degradation in performance in some situations.

\begin{figure}
  \begin{subfigure}{0.5\textwidth}
    \centering
    \caption{GPT-4o}
    \label{fig:gpt-4o-few-shot}
    \includegraphics[scale=0.30]{assets/gpt-4o-n-shot-variation-ontology-map.pdf}
  \end{subfigure}
  \hspace{0.2cm}
  \begin{subfigure}{0.5\textwidth}
    \centering
    \caption{Llama3-8B}
    \label{fig:llama3-few-shot}
    \includegraphics[scale=0.30]{assets/Llama3-7B-n-shot-variation-ontology-map.pdf}
  \end{subfigure}
  \caption{mAP@D variation with number of examples for GPT-4o (a) and
    Llama3-8B (b) using ontology domain contextualization}\label{fig:few-shot}
\end{figure}

%% Zero-shot vs. Few-shot
Including a single example leads to significant performance improvements,
particularly on the CASE ontology. Significant improvements in $mAP@D$
highlight how LLMs can adapt with little context. Improvements in $mAP@1$ are
less pronounced in comparison.
% Thus, the primary benefit of adding examples is in identifying the
% ontological hierarchy relations that a user is looking for.
Thus, adding examples primarily improves an LLM's ability to infer the correct
ontology concept hierarchy.  The performance improvements on the Wines and
Astronomy ontologies are less pronounced, possibly due to the already
compatible semantic understanding the LLMs possess of their entities.

%% Few-shot variation
Figure \ref{fig:few-shot} highlights that adding more examples yields
continued performance improvements that plateaus at 3 - 4 examples.
Performance gains from adding additional examples is more pronounced for
Llama3-8B compared to GPT-4o.  This suggests that smaller LLMs benefit more
from seeing more examples.  Experimentation with zero-shot and few-shot
strategies highlights that few-shot prompting can provide significant
performance improvements for ontology population, particularly when an LLM's
entity label semantics differs from the ontological entity properties. Largest
performance benefits are observed when using 3 or 4 examples beyond which
smaller LLMs may benefit more.

Of the $72$ ($288 / 4$) domain contextualization experimentation groups (each
combination of LLM, ontology and prompting strategy, over four types of domain
contextualization together comprise one group), ontology contextualization
yielded the best performance for 31 groups ($43.1\%$) followed by ontology and
topic (double) contextualization for 21 groups ($29.2\%$). From these 52
groups with ontology or double contextualization as the best performer, in 27
groups, the second-best performer was the other contextualization method. Of
the remaining 20 groups, ontology or double contextualization was second-best
in 12 groups. This clearly highlights the preference for the taxonomical
contextualization variants. Topic contextualization provided the best
performance in 16 groups ($22.2\%$).  Table \ref{tab:results} reports the
ontology domain contextualization scores for each ontology and LLM under
different prompting strategies.  We observed the difference between the best
and second-best strategies to be marginal ($< 0.017$) in several groups.

\begin{figure}
  \begin{subfigure}{0.5\textwidth}
    \centering
    \caption{Wines Ontology}
    \label{fig:wines-temp-variation}
    \includegraphics[scale=0.32]{assets/wines-ontology-temp-stat-variation-ontology.pdf}
  \end{subfigure}
  \hspace{0.2cm}
  \begin{subfigure}{0.5\textwidth}
    \centering
    \caption{CASE Ontology}
    \label{fig:case-temp-variation}
    \includegraphics[scale=0.32]{assets/case-uco-owl-trafficking-temp-stat-variation-ontology.pdf}
  \end{subfigure}
  \caption{Temperature variation for (a) Wines and (b) CASE ontologies for
    GPT-4o and Llama3-8B with 3-shot, ontology domain
    contextualization. Central points are the average value over 10
    runs. Lighter shaded regions are approximated densities over the range of
    values}
  \label{fig:temp-variation}
\end{figure}

LLMs are inherently non-deterministic tools. The results they produce are not
guaranteed to be 100\% replicable even from identical prompts. Hence, when
investigating the applicability of LLMs for the task of determining semantic
relations between entities, it is important to examine the potential variance
in their performance under fixed conditions. A relevant question is whether
calibration of the LLM temperature parameter can improve the mean performance
or reduce its variance.

% 
Figure \ref{fig:temp-variation} presents $mAP@D$ variability exhibited by
GPT-4o and Llama3-8B models run 10 times over Wines and CASE ontologies under
three different temperature settings and with all other parameters fixed.

GPT-4o's results are fairly consistent across all runs, which is observable as
fairly small spreads in $mAP@D$ values for all three temperature settings and
both ontologies. In contrast, there is more variability in the $mAP@D$ scores
for Llama3-8B. Surprisingly enough, neither higher temperature settings nor
ontology differences bring about peaks in variance (although, more exploration
of this issue is required).  Looking at the average scores and the influence
of temperature on them, we observed no difference in performance at the low
temperature setting. Results of the four Welch's T-tests carried out to
compare the average $mAP@D$ scores produced by the same model over the same
ontology at the low and default temperature conditions are
non-significant. Raising the temperature seems to have a detrimental
effect. For the Wines ontology, GPT-4o produces a significantly higher average
$mAP@D$ score under default temperature conditions (M = 0.88, SD = 0.01) than
under the high temperature setting (M = 0.85, SD = 0.01), t(18) = 6.6, p <
.001. In a similar fashion, for the CASE ontology, the default temperature
resulted in a significantly higher average $mAP@D$ score as well (M = 0.76, SD
= 0.01) compared to the higher temperature (M = 0.71, SD = 0.01), t(18) = 9.8,p < .001. Although the same trend is observable for Llama3-8B, the much higher
variance of the smaller LLM's results renders the test non-significant.

\section{Conclusion}
\label{sec:conclusion}


\bibliography{bibliography}
\end{document}
